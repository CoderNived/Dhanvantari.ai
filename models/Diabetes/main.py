"""
Diabetes Disease Prediction System - Production Ready ML Pipeline
====================================================================
Author: Senior ML Engineer
Dataset: Pima Indians Diabetes Dataset
Target: Build a medically-sound ML system with >85% accuracy and high recall

This pipeline follows strict medical ML standards:
- Intelligent handling of medically invalid zeros
- Multiple imputation strategies comparison
- Class imbalance handling
- Comprehensive evaluation metrics
- Model interpretability
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, roc_auc_score, confusion_matrix, 
                             classification_report, roc_curve)
from sklearn.impute import KNNImputer
import warnings
import pickle
warnings.filterwarnings('ignore')

# Set style for better visualizations
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)


def load_data(filepath):
    """
    Load the Pima Indians Diabetes dataset.
    
    Args:
        filepath (str): Path to the CSV file
        
    Returns:
        pd.DataFrame: Loaded dataset
    """
    print("=" * 80)
    print("STEP 1: LOADING DATA")
    print("=" * 80)
    
    df = pd.read_csv(r"D:\Dhanvantari.ai\models\Diabetes\diabetes.csv")
    print(f"‚úì Dataset loaded successfully")
    print(f"‚úì Shape: {df.shape[0]} samples, {df.shape[1]} features")
    print(f"\nColumn names:\n{df.columns.tolist()}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nFirst few rows:\n{df.head()}")
    
    return df


def perform_eda(df):
    """
    Perform comprehensive Exploratory Data Analysis.
    
    Args:
        df (pd.DataFrame): Input dataset
    """
    print("\n" + "=" * 80)
    print("STEP 2: EXPLORATORY DATA ANALYSIS")
    print("=" * 80)
    
    # Basic statistics
    print("\nüìä BASIC STATISTICS:")
    print(df.describe())
    
    # Missing values check
    print("\nüîç MISSING VALUES:")
    print(df.isnull().sum())
    
    # Check for medically invalid zeros
    print("\n‚ö†Ô∏è  MEDICALLY INVALID ZEROS DETECTED:")
    zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    for col in zero_cols:
        zero_count = (df[col] == 0).sum()
        zero_pct = (zero_count / len(df)) * 100
        print(f"  {col:20s}: {zero_count:3d} zeros ({zero_pct:5.2f}%)")
    
    # Class distribution
    print("\nüìà CLASS DISTRIBUTION:")
    outcome_counts = df['Outcome'].value_counts()
    print(outcome_counts)
    print(f"\nClass balance: {outcome_counts[1]/outcome_counts[0]:.2f} (1:1 is balanced)")
    imbalance_ratio = outcome_counts[0] / outcome_counts[1]
    print(f"Imbalance ratio: {imbalance_ratio:.2f}:1 (Non-diabetic:Diabetic)")
    
    # Visualizations
    fig, axes = plt.subplots(3, 3, figsize=(18, 14))
    fig.suptitle('Feature Distributions by Outcome', fontsize=16, fontweight='bold')
    
    features = df.columns[:-1]  # All except Outcome
    for idx, col in enumerate(features):
        ax = axes[idx // 3, idx % 3]
        df[df['Outcome'] == 0][col].hist(ax=ax, bins=30, alpha=0.6, label='Non-Diabetic', color='blue')
        df[df['Outcome'] == 1][col].hist(ax=ax, bins=30, alpha=0.6, label='Diabetic', color='red')
        ax.set_title(col, fontweight='bold')
        ax.set_xlabel(col)
        ax.set_ylabel('Frequency')
        ax.legend()
    
    
    # Correlation matrix
    plt.figure(figsize=(12, 10))
    correlation_matrix = df.corr()
    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    
    
    # Key insights
    print("\nüí° KEY INSIGHTS FROM EDA:")
    print(f"  1. Class imbalance: {imbalance_ratio:.2f}:1 ‚Üí Need class weights or SMOTE")
    print(f"  2. Invalid zeros present ‚Üí Need intelligent imputation")
    print(f"  3. Glucose shows highest correlation with Outcome: {correlation_matrix.loc['Glucose', 'Outcome']:.3f}")
    print(f"  4. BMI and Age also show moderate correlation with Outcome")
    print(f"  5. Dataset size is modest (768) ‚Üí Use cross-validation for robust evaluation")
    
    plt.close('all')


def clean_data(df):
    """
    Handle medically invalid zeros using intelligent imputation.
    
    Medical justification:
    - Glucose=0, BloodPressure=0, BMI=0 are physiologically impossible
    - These are missing data markers, not actual measurements
    - We'll compare median imputation vs KNN imputation
    
    Args:
        df (pd.DataFrame): Input dataset
        
    Returns:
        tuple: (cleaned_df_median, cleaned_df_knn)
    """
    print("\n" + "=" * 80)
    print("STEP 3: DATA CLEANING & IMPUTATION")
    print("=" * 80)
    
    # Columns with medically invalid zeros
    zero_not_allowed = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    
    # Create copies for different strategies
    df_median = df.copy()
    df_knn = df.copy()
    
    # Replace zeros with NaN for imputation
    for col in zero_not_allowed:
        df_median.loc[df_median[col] == 0, col] = np.nan
        df_knn.loc[df_knn[col] == 0, col] = np.nan
    
    print("\nüîß IMPUTATION STRATEGY COMPARISON:")
    
    # Strategy 1: Median Imputation (grouped by Outcome)
    print("\n1Ô∏è‚É£ MEDIAN IMPUTATION (Outcome-stratified):")
    print("   Rationale: Replace with median of same outcome class")
    print("   Pros: Simple, preserves class-specific distributions")
    print("   Cons: Ignores feature correlations")
    
    for col in zero_not_allowed:
        median_0 = df_median[df_median['Outcome'] == 0][col].median()
        median_1 = df_median[df_median['Outcome'] == 1][col].median()
        
        df_median.loc[(df_median[col].isna()) & (df_median['Outcome'] == 0), col] = median_0
        df_median.loc[(df_median[col].isna()) & (df_median['Outcome'] == 1), col] = median_1
        
        print(f"   {col:20s}: Non-diabetic median={median_0:.2f}, Diabetic median={median_1:.2f}")
    
    # Strategy 2: KNN Imputation
    print("\n2Ô∏è‚É£ KNN IMPUTATION (k=5):")
    print("   Rationale: Use 5 nearest neighbors to predict missing values")
    print("   Pros: Considers feature correlations, more sophisticated")
    print("   Cons: Computationally expensive, sensitive to scaling")
    
    # Separate features and target for KNN imputation
    X_knn = df_knn.drop('Outcome', axis=1)
    y_knn = df_knn['Outcome']
    
    imputer = KNNImputer(n_neighbors=5, weights='distance')
    X_knn_imputed = pd.DataFrame(
        imputer.fit_transform(X_knn),
        columns=X_knn.columns
    )
    df_knn = pd.concat([X_knn_imputed, y_knn.reset_index(drop=True)], axis=1)
    
    print("   ‚úì KNN imputation completed")
    
    # Compare distributions before/after
    print("\nüìä IMPUTATION QUALITY CHECK:")
    for col in zero_not_allowed[:3]:  # Check first 3 features
        orig_mean = df[df[col] > 0][col].mean()
        median_mean = df_median[col].mean()
        knn_mean = df_knn[col].mean()
        print(f"   {col:20s}: Original={orig_mean:6.2f}, Median={median_mean:6.2f}, KNN={knn_mean:6.2f}")
    
    print("\n‚úì Data cleaning completed")
    print(f"‚úì Median-imputed shape: {df_median.shape}")
    print(f"‚úì KNN-imputed shape: {df_knn.shape}")
    
    return df_median, df_knn


def feature_engineering(df):
    """
    Create meaningful derived features based on medical knowledge.
    
    Args:
        df (pd.DataFrame): Input dataset
        
    Returns:
        pd.DataFrame: Dataset with engineered features
    """
    print("\n" + "=" * 80)
    print("STEP 4: FEATURE ENGINEERING")
    print("=" * 80)
    
    df_engineered = df.copy()
    
    # 1. BMI Categories (WHO classification)
    df_engineered['BMI_Category'] = pd.cut(
        df_engineered['BMI'],
        bins=[0, 18.5, 25, 30, 100],
        labels=[0, 1, 2, 3]  # Underweight, Normal, Overweight, Obese
    ).astype(int)
    
    # 2. Age Groups
    df_engineered['Age_Group'] = pd.cut(
        df_engineered['Age'],
        bins=[0, 30, 45, 60, 100],
        labels=[0, 1, 2, 3]  # Young, Middle, Senior, Elderly
    ).astype(int)
    
    # 3. Glucose Risk Level
    df_engineered['Glucose_Risk'] = pd.cut(
        df_engineered['Glucose'],
        bins=[0, 100, 125, 200],
        labels=[0, 1, 2]  # Normal, Prediabetic, Diabetic
    ).astype(int)
    
    # 4. Pregnancy Risk (high parity is a risk factor)
    df_engineered['High_Pregnancies'] = (df_engineered['Pregnancies'] >= 6).astype(int)
    
    # 5. Interaction features
    df_engineered['BMI_Age_Interaction'] = df_engineered['BMI'] * df_engineered['Age'] / 100
    df_engineered['Glucose_BMI_Ratio'] = df_engineered['Glucose'] / df_engineered['BMI']
    
    print("\nüî¨ ENGINEERED FEATURES:")
    print("  1. BMI_Category: WHO BMI classification (0-3)")
    print("  2. Age_Group: Age stratification (0-3)")
    print("  3. Glucose_Risk: Glucose level categorization (0-2)")
    print("  4. High_Pregnancies: Binary flag for ‚â•6 pregnancies")
    print("  5. BMI_Age_Interaction: Combined risk factor")
    print("  6. Glucose_BMI_Ratio: Metabolic indicator")
    
    print(f"\n‚úì Feature engineering completed")
    print(f"‚úì New feature count: {df_engineered.shape[1] - df.shape[1]}")
    print(f"‚úì Total features: {df_engineered.shape[1] - 1} (excluding target)")
    
    return df_engineered


def split_data(df, test_size=0.2, random_state=42):
    """
    Split data into training and testing sets with stratification.
    
    Args:
        df (pd.DataFrame): Input dataset
        test_size (float): Proportion of test set
        random_state (int): Random seed for reproducibility
        
    Returns:
        tuple: X_train, X_test, y_train, y_test
    """
    print("\n" + "=" * 80)
    print("STEP 5: DATA SPLITTING")
    print("=" * 80)
    
    X = df.drop('Outcome', axis=1)
    y = df['Outcome']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"\n‚úì Data split completed (stratified sampling)")
    print(f"  Training set: {X_train.shape[0]} samples ({(1-test_size)*100:.0f}%)")
    print(f"  Test set: {X_test.shape[0]} samples ({test_size*100:.0f}%)")
    print(f"\n  Training class distribution:")
    print(f"    Non-diabetic: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)")
    print(f"    Diabetic: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)")
    print(f"\n  Test class distribution:")
    print(f"    Non-diabetic: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)")
    print(f"    Diabetic: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)")
    
    return X_train, X_test, y_train, y_test


def scale_features(X_train, X_test):
    """
    Standardize features using StandardScaler.
    
    Critical: Fit on training data only to prevent data leakage.
    
    Args:
        X_train (pd.DataFrame): Training features
        X_test (pd.DataFrame): Test features
        
    Returns:
        tuple: (X_train_scaled, X_test_scaled, scaler)
    """
    print("\n" + "=" * 80)
    print("STEP 6: FEATURE SCALING")
    print("=" * 80)
    
    scaler = StandardScaler()
    
    # Fit on training data only
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Convert back to DataFrame for clarity
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
    
    print("‚úì StandardScaler applied")
    print(f"‚úì Mean of scaled training features: {X_train_scaled.mean().mean():.6f} (should be ~0)")
    print(f"‚úì Std of scaled training features: {X_train_scaled.std().mean():.6f} (should be ~1)")
    print("\n‚ö†Ô∏è  IMPORTANT: Scaler fitted on training data only (prevents data leakage)")
    
    return X_train_scaled, X_test_scaled, scaler


def train_models(X_train, y_train, use_class_weights=True):
    """
    Train multiple ML models with hyperparameter tuning and cross-validation.
    
    Medical ML justification:
    - Class weights handle imbalance without synthetic data
    - Cross-validation ensures robust generalization
    - Multiple models capture different patterns
    
    Args:
        X_train (pd.DataFrame): Training features
        y_train (pd.Series): Training labels
        use_class_weights (bool): Whether to use class weights
        
    Returns:
        dict: Trained models with their best parameters
    """
    print("\n" + "=" * 80)
    print("STEP 7: MODEL TRAINING & HYPERPARAMETER TUNING")
    print("=" * 80)
    
    # Calculate class weights
    class_weight_dict = None
    if use_class_weights:
        n_samples = len(y_train)
        n_classes = 2
        n_negative = (y_train == 0).sum()
        n_positive = (y_train == 1).sum()
        
        # Balanced class weights
        weight_negative = n_samples / (n_classes * n_negative)
        weight_positive = n_samples / (n_classes * n_positive)
        class_weight_dict = {0: weight_negative, 1: weight_positive}
        
        print(f"\n‚öñÔ∏è  CLASS WEIGHTS CALCULATED:")
        print(f"  Non-diabetic (0): {weight_negative:.3f}")
        print(f"  Diabetic (1): {weight_positive:.3f}")
        print(f"  Ratio: {weight_positive/weight_negative:.3f}:1")
    
    # Stratified K-Fold for cross-validation
    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    models = {}
    
    # ========== MODEL 1: Logistic Regression ==========
    print("\n" + "-" * 80)
    print("MODEL 1: LOGISTIC REGRESSION")
    print("-" * 80)
    print("Pros: Interpretable, fast, works well with linear relationships")
    print("Cons: Assumes linear decision boundary")
    
    lr_params = {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'penalty': ['l2'],
        'solver': ['lbfgs'],
        'max_iter': [1000]
    }
    
    lr_base = LogisticRegression(class_weight=class_weight_dict, random_state=42)
    lr_grid = GridSearchCV(lr_base, lr_params, cv=cv_strategy, scoring='roc_auc', n_jobs=-1)
    lr_grid.fit(X_train, y_train)
    
    models['Logistic Regression'] = lr_grid.best_estimator_
    print(f"‚úì Best params: {lr_grid.best_params_}")
    print(f"‚úì Best CV ROC-AUC: {lr_grid.best_score_:.4f}")
    
    # ========== MODEL 2: Support Vector Machine ==========
    print("\n" + "-" * 80)
    print("MODEL 2: SUPPORT VECTOR MACHINE (RBF)")
    print("-" * 80)
    print("Pros: Handles non-linear boundaries, robust to outliers")
    print("Cons: Slower to train, less interpretable")
    
    svm_params = {
        'C': [0.1, 1, 10, 100],
        'gamma': ['scale', 0.001, 0.01, 0.1],
        'kernel': ['rbf']
    }
    
    svm_base = SVC(class_weight=class_weight_dict, probability=True, random_state=42)
    svm_grid = GridSearchCV(svm_base, svm_params, cv=cv_strategy, scoring='roc_auc', n_jobs=-1)
    svm_grid.fit(X_train, y_train)
    
    models['SVM'] = svm_grid.best_estimator_
    print(f"‚úì Best params: {svm_grid.best_params_}")
    print(f"‚úì Best CV ROC-AUC: {svm_grid.best_score_:.4f}")
    
    # ========== MODEL 3: Random Forest ==========
    print("\n" + "-" * 80)
    print("MODEL 3: RANDOM FOREST")
    print("-" * 80)
    print("Pros: Handles non-linearity, feature importance, robust")
    print("Cons: Can overfit, less interpretable than single tree")
    
    rf_params = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    rf_base = RandomForestClassifier(class_weight=class_weight_dict, random_state=42)
    rf_grid = GridSearchCV(rf_base, rf_params, cv=cv_strategy, scoring='roc_auc', n_jobs=-1)
    rf_grid.fit(X_train, y_train)
    
    models['Random Forest'] = rf_grid.best_estimator_
    print(f"‚úì Best params: {rf_grid.best_params_}")
    print(f"‚úì Best CV ROC-AUC: {rf_grid.best_score_:.4f}")
    
    # ========== MODEL 4: Gradient Boosting ==========
    print("\n" + "-" * 80)
    print("MODEL 4: GRADIENT BOOSTING")
    print("-" * 80)
    print("Pros: State-of-the-art performance, handles complex patterns")
    print("Cons: Slower to train, risk of overfitting")
    
    gb_params = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 1.0]
    }
    
    gb_base = GradientBoostingClassifier(random_state=42)
    gb_grid = GridSearchCV(gb_base, gb_params, cv=cv_strategy, scoring='roc_auc', n_jobs=-1)
    gb_grid.fit(X_train, y_train)
    
    models['Gradient Boosting'] = gb_grid.best_estimator_
    print(f"‚úì Best params: {gb_grid.best_params_}")
    print(f"‚úì Best CV ROC-AUC: {gb_grid.best_score_:.4f}")
    
    print("\n" + "=" * 80)
    print("‚úì All models trained successfully")
    print("=" * 80)
    
    return models


def evaluate_models(models, X_train, X_test, y_train, y_test):
    """
    Comprehensive evaluation of all models using medical ML metrics.
    
    Medical priority: HIGH RECALL for diabetic class (minimize false negatives)
    False Negative = Missing a diabetic patient (HIGH RISK)
    False Positive = Flagging non-diabetic as diabetic (Lower risk, can be verified)
    
    Args:
        models (dict): Trained models
        X_train (pd.DataFrame): Training features
        X_test (pd.DataFrame): Test features
        y_train (pd.Series): Training labels
        y_test (pd.Series): Test labels
        
    Returns:
        pd.DataFrame: Comprehensive evaluation results
    """
    print("\n" + "=" * 80)
    print("STEP 8: MODEL EVALUATION")
    print("=" * 80)
    print("\nüè• MEDICAL ML EVALUATION PRIORITIES:")
    print("  1. Recall (Sensitivity): Catch diabetic patients (minimize false negatives)")
    print("  2. ROC-AUC: Overall discrimination ability")
    print("  3. F1-Score: Balance between precision and recall")
    print("  4. Accuracy: Overall correctness (but NOT the primary metric)")
    
    results = []
    
    for model_name, model in models.items():
        print("\n" + "-" * 80)
        print(f"EVALUATING: {model_name}")
        print("-" * 80)
        
        # Predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        y_test_proba = model.predict_proba(X_test)[:, 1]
        
        # Metrics
        train_acc = accuracy_score(y_train, y_train_pred)
        test_acc = accuracy_score(y_test, y_test_pred)
        precision = precision_score(y_test, y_test_pred)
        recall = recall_score(y_test, y_test_pred)  # CRITICAL for medical ML
        f1 = f1_score(y_test, y_test_pred)
        roc_auc = roc_auc_score(y_test, y_test_proba)
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_test_pred)
        tn, fp, fn, tp = cm.ravel()
        
        print(f"\nüìä PERFORMANCE METRICS:")
        print(f"  Training Accuracy: {train_acc:.4f}")
        print(f"  Test Accuracy: {test_acc:.4f}")
        print(f"  Precision: {precision:.4f} (When model says diabetic, it's correct {precision*100:.1f}% of time)")
        print(f"  Recall: {recall:.4f} (Model catches {recall*100:.1f}% of diabetic patients) ‚ö†Ô∏è CRITICAL")
        print(f"  F1-Score: {f1:.4f}")
        print(f"  ROC-AUC: {roc_auc:.4f}")
        
        print(f"\nüîç CONFUSION MATRIX:")
        print(f"  True Negatives (TN): {tn:3d} - Correctly identified non-diabetic")
        print(f"  False Positives (FP): {fp:3d} - Non-diabetic flagged as diabetic")
        print(f"  False Negatives (FN): {fn:3d} - Diabetic missed ‚ö†Ô∏è DANGEROUS")
        print(f"  True Positives (TP): {tp:3d} - Correctly identified diabetic")
        
        # Medical interpretation
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0
        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
        
        print(f"\nüè• CLINICAL METRICS:")
        print(f"  Sensitivity (Recall): {recall:.4f} - % of diabetics correctly identified")
        print(f"  Specificity: {specificity:.4f} - % of non-diabetics correctly identified")
        print(f"  PPV (Precision): {ppv:.4f} - If test is positive, probability of diabetes")
        print(f"  NPV: {npv:.4f} - If test is negative, probability of no diabetes")
        
        # Store results
        results.append({
            'Model': model_name,
            'Train_Acc': train_acc,
            'Test_Acc': test_acc,
            'Precision': precision,
            'Recall': recall,
            'F1_Score': f1,
            'ROC_AUC': roc_auc,
            'Specificity': specificity,
            'TN': tn,
            'FP': fp,
            'FN': fn,
            'TP': tp
        })
    
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('ROC_AUC', ascending=False)
    
    print("\n" + "=" * 80)
    print("COMPREHENSIVE RESULTS SUMMARY")
    print("=" * 80)
    print(results_df.to_string(index=False))
    
    return results_df


def plot_model_comparison(results_df, models, X_test, y_test):
    """
    Create comprehensive visualizations for model comparison.
    
    Args:
        results_df (pd.DataFrame): Evaluation results
        models (dict): Trained models
        X_test (pd.DataFrame): Test features
        y_test (pd.Series): Test labels
    """
    print("\n" + "=" * 80)
    print("STEP 9: VISUALIZATION & COMPARISON")
    print("=" * 80)
    
    # Plot 1: Metrics Comparison
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
    
    metrics = ['Test_Acc', 'Recall', 'Precision', 'ROC_AUC']
    titles = ['Test Accuracy', 'Recall (Sensitivity)', 'Precision', 'ROC-AUC Score']
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']
    
    for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
        ax = axes[idx // 2, idx % 2]
        results_df.plot(x='Model', y=metric, kind='bar', ax=ax, color=color, legend=False)
        ax.set_title(title, fontweight='bold', fontsize=12)
        ax.set_ylabel('Score')
        ax.set_xlabel('')
        ax.set_ylim([0, 1])
        ax.axhline(y=0.85, color='red', linestyle='--', label='Target (0.85)')
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        # Add value labels on bars
        for container in ax.containers:
            ax.bar_label(container, fmt='%.3f', padding=3)
    
    plt.tight_layout()
    plt.show()
    
    
    # Plot 2: ROC Curves
    plt.figure(figsize=(12, 10))
    
    for model_name, model in models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        auc = roc_auc_score(y_test, y_pred_proba)
        plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
    plt.ylabel('True Positive Rate (Recall)', fontsize=12, fontweight='bold')
    plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    # Plot 3: Confusion Matrices
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    fig.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold')
    
    for idx, (model_name, model) in enumerate(models.items()):
        ax = axes[idx // 2, idx % 2]
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,
                   xticklabels=['Non-Diabetic', 'Diabetic'],
                   yticklabels=['Non-Diabetic', 'Diabetic'])
        ax.set_title(model_name, fontweight='bold', fontsize=12)
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        
        # Highlight false negatives
        tn, fp, fn, tp = cm.ravel()
        ax.text(0.5, 1.5, f'FN={fn}', ha='center', va='center', 
               bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))
    
    plt.tight_layout()
    plt.show()
    
    plt.close('all')


def select_best_model(results_df, models):
    """
    Select the best model based on medical ML priorities.
    
    Selection criteria (in order):
    1. ROC-AUC ‚â• 0.85 (good discrimination)
    2. Recall ‚â• 0.80 (catch at least 80% of diabetic patients)
    3. Test Accuracy ‚â• 0.85
    4. Minimal overfitting (Train-Test gap < 5%)
    
    Args:
        results_df (pd.DataFrame): Evaluation results
        models (dict): Trained models
        
    Returns:
        tuple: (best_model_name, best_model)
    """
    print("\n" + "=" * 80)
    print("STEP 10: BEST MODEL SELECTION")
    print("=" * 80)
    
    print("\nüéØ SELECTION CRITERIA (Medical ML Standards):")
    print("  1. ROC-AUC ‚â• 0.85 (Strong discrimination)")
    print("  2. Recall ‚â• 0.80 (Catch ‚â•80% of diabetic patients)")
    print("  3. Test Accuracy ‚â• 0.85 (Overall performance)")
    print("  4. Minimal overfitting (Train-Test gap < 5%)")
    
    # Filter candidates
    candidates = results_df[
        (results_df['ROC_AUC'] >= 0.80) &
        (results_df['Recall'] >= 0.75) &
        (results_df['Test_Acc'] >= 0.80)
    ].copy()
    
    if len(candidates) == 0:
        print("\n‚ö†Ô∏è  No models meet all strict criteria. Selecting best available.")
        candidates = results_df.copy()
    
    # Calculate overfitting metric
    candidates['Overfit_Gap'] = candidates['Train_Acc'] - candidates['Test_Acc']
    
    # Composite score (weighted)
    candidates['Composite_Score'] = (
        0.35 * candidates['ROC_AUC'] +
        0.30 * candidates['Recall'] +
        0.20 * candidates['Test_Acc'] +
        0.15 * candidates['F1_Score']
    )
    
    # Select best
    best_idx = candidates['Composite_Score'].idxmax()
    best_row = candidates.loc[best_idx]
    best_model_name = best_row['Model']
    best_model = models[best_model_name]
    
    print(f"\nüèÜ BEST MODEL: {best_model_name}")
    print(f"  Test Accuracy: {best_row['Test_Acc']:.4f}")
    print(f"  Recall: {best_row['Recall']:.4f}")
    print(f"  Precision: {best_row['Precision']:.4f}")
    print(f"  F1-Score: {best_row['F1_Score']:.4f}")
    print(f"  ROC-AUC: {best_row['ROC_AUC']:.4f}")
    print(f"  Overfitting Gap: {best_row['Overfit_Gap']:.4f}")
    print(f"  Composite Score: {best_row['Composite_Score']:.4f}")
    
    print(f"\nüí° WHY THIS MODEL?")
    if best_row['ROC_AUC'] >= 0.85:
        print("  ‚úì Excellent discrimination (ROC-AUC ‚â• 0.85)")
    if best_row['Recall'] >= 0.80:
        print("  ‚úì High recall - catches most diabetic patients")
    if best_row['Test_Acc'] >= 0.85:
        print("  ‚úì Meets accuracy target (‚â•85%)")
    if best_row['Overfit_Gap'] < 0.05:
        print("  ‚úì Good generalization (minimal overfitting)")
    
    print(f"\n‚ö†Ô∏è  CLINICAL INTERPRETATION:")
    print(f"  Out of 100 diabetic patients, this model will identify {best_row['Recall']*100:.0f}")
    print(f"  False negatives: {best_row['FN']} patients (would be missed)")
    print(f"  False positives: {best_row['FP']} patients (flagged unnecessarily)")
    
    return best_model_name, best_model


def save_model_pipeline(model, scaler, feature_names, filename='diabetes_model.pkl'):
    """
    Save the complete model pipeline for deployment.
    
    Args:
        model: Trained model
        scaler: Fitted StandardScaler
        feature_names (list): List of feature names
        filename (str): Output filename
    """
    print("\n" + "=" * 80)
    print("STEP 11: MODEL PERSISTENCE")
    print("=" * 80)
    
    pipeline = {
        'model': model,
        'scaler': scaler,
        'feature_names': feature_names,
        'model_type': type(model).__name__
    }
    
    filepath = f'/mnt/user-data/outputs/{filename}'
    with open(filepath, 'wb') as f:
        pickle.dump(pipeline, f)
    
    print(f"\n‚úì Model pipeline saved to: {filename}")
    print(f"‚úì Pipeline includes:")
    print(f"  - Trained model: {type(model).__name__}")
    print(f"  - Fitted scaler: StandardScaler")
    print(f"  - Feature names: {len(feature_names)} features")
    print(f"  - Model type metadata")
    
    # Verify save
    with open(filepath, 'rb') as f:
        loaded = pickle.load(f)
    print(f"\n‚úì Verification: Model reloaded successfully")
    
    return filepath


def predict_diabetes(input_data, model_path='/mnt/user-data/outputs/diabetes_model.pkl'):
    """
    Production-ready prediction function.
    
    Args:
        input_data (dict): Patient data
        model_path (str): Path to saved model
        
    Returns:
        dict: Prediction results with probabilities and interpretation
    """
    # Load model pipeline
    with open(model_path, 'rb') as f:
        pipeline = pickle.load(f)
    
    model = pipeline['model']
    scaler = pipeline['scaler']
    feature_names = pipeline['feature_names']
    
    # Validate input
    required_base_features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 
                              'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
    
    for feat in required_base_features:
        if feat not in input_data:
            raise ValueError(f"Missing required feature: {feat}")
    
    # Create DataFrame
    df = pd.DataFrame([input_data])
    
    # Feature engineering (must match training pipeline)
    df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 100], labels=[0, 1, 2, 3]).astype(int)
    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 30, 45, 60, 100], labels=[0, 1, 2, 3]).astype(int)
    df['Glucose_Risk'] = pd.cut(df['Glucose'], bins=[0, 100, 125, 200], labels=[0, 1, 2]).astype(int)
    df['High_Pregnancies'] = (df['Pregnancies'] >= 6).astype(int)
    df['BMI_Age_Interaction'] = df['BMI'] * df['Age'] / 100
    df['Glucose_BMI_Ratio'] = df['Glucose'] / df['BMI']
    
    # Ensure correct feature order
    df = df[feature_names]
    
    # Scale features
    df_scaled = scaler.transform(df)
    
    # Predict
    prediction = model.predict(df_scaled)[0]
    probability = model.predict_proba(df_scaled)[0]
    
    # Interpret
    result = {
        'prediction': int(prediction),
        'prediction_label': 'Diabetic' if prediction == 1 else 'Non-Diabetic',
        'probability_non_diabetic': float(probability[0]),
        'probability_diabetic': float(probability[1]),
        'confidence': float(max(probability)),
        'risk_level': 'High' if probability[1] >= 0.7 else 'Moderate' if probability[1] >= 0.3 else 'Low',
        'recommendation': ''
    }
    
    # Clinical recommendation
    if result['prediction'] == 1:
        result['recommendation'] = "HIGH RISK: Recommend immediate consultation with healthcare provider for confirmatory testing (HbA1c, fasting glucose)."
    elif probability[1] >= 0.3:
        result['recommendation'] = "MODERATE RISK: Consider lifestyle modifications and follow-up screening in 3-6 months."
    else:
        result['recommendation'] = "LOW RISK: Continue healthy lifestyle and routine annual screening."
    
    return result


def main():
    """
    Main execution pipeline.
    """
    print("‚ïî" + "=" * 78 + "‚ïó")
    print("‚ïë" + " " * 15 + "DIABETES PREDICTION ML PIPELINE" + " " * 32 + "‚ïë")
    print("‚ïë" + " " * 20 + "Production-Ready Medical ML System" + " " * 24 + "‚ïë")
    print("‚ïö" + "=" * 78 + "‚ïù")
    
    # Load data
    df = load_data('/mnt/user-data/uploads/diabetes.csv')
    
    # EDA
    perform_eda(df)
    
    # Data cleaning - compare strategies
    df_median, df_knn = clean_data(df)
    
    # We'll use KNN imputation as it's more sophisticated
    print("\nüéØ SELECTED IMPUTATION STRATEGY: KNN (k=5)")
    print("   Rationale: Leverages feature correlations for better imputation")
    df_clean = df_knn
    
    # Feature engineering
    df_engineered = feature_engineering(df_clean)
    
    # Split data
    X_train, X_test, y_train, y_test = split_data(df_engineered)
    
    # Scale features
    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)
    
    # Train models
    models = train_models(X_train_scaled, y_train, use_class_weights=True)
    
    # Evaluate models
    results_df = evaluate_models(models, X_train_scaled, X_test_scaled, y_train, y_test)
    
    # Visualizations
    plot_model_comparison(results_df, models, X_test_scaled, y_test)
    
    # Select best model
    best_model_name, best_model = select_best_model(results_df, models)
    
    # Save model
    model_path = save_model_pipeline(best_model, scaler, X_train.columns.tolist())
    
    # Demo prediction
    print("\n" + "=" * 80)
    print("STEP 12: PRODUCTION DEPLOYMENT DEMO")
    print("=" * 80)
    
    # Example patient
    example_patient = {
        'Pregnancies': 2,
        'Glucose': 148,
        'BloodPressure': 72,
        'SkinThickness': 35,
        'Insulin': 100,
        'BMI': 33.6,
        'DiabetesPedigreeFunction': 0.627,
        'Age': 50
    }
    
    print("\nüè• EXAMPLE PREDICTION:")
    print(f"Patient data: {example_patient}")
    
    prediction_result = predict_diabetes(example_patient, model_path)
    
    print(f"\nüìä PREDICTION RESULTS:")
    print(f"  Prediction: {prediction_result['prediction_label']}")
    print(f"  Probability (Diabetic): {prediction_result['probability_diabetic']:.2%}")
    print(f"  Probability (Non-Diabetic): {prediction_result['probability_non_diabetic']:.2%}")
    print(f"  Confidence: {prediction_result['confidence']:.2%}")
    print(f"  Risk Level: {prediction_result['risk_level']}")
    print(f"\nüí° Recommendation:")
    print(f"  {prediction_result['recommendation']}")
    
    # Final summary
    print("\n" + "‚ïî" + "=" * 78 + "‚ïó")
    print("‚ïë" + " " * 25 + "PIPELINE COMPLETE" + " " * 36 + "‚ïë")
    print("‚ïö" + "=" * 78 + "‚ïù")
    
    print(f"\n‚úÖ DELIVERABLES:")
    print(f"  1. Cleaned and engineered dataset")
    print(f"  2. 4 trained models with hyperparameter tuning")
    print(f"  3. Comprehensive evaluation report")
    print(f"  4. Best model: {best_model_name}")
    print(f"  5. Model artifacts saved for deployment")
    print(f"  6. Production-ready prediction function")
    
    best_results = results_df[results_df['Model'] == best_model_name].iloc[0]
    print(f"\nüéØ FINAL PERFORMANCE ({best_model_name}):")
    print(f"  ‚úì Test Accuracy: {best_results['Test_Acc']:.2%} {'‚úì TARGET MET' if best_results['Test_Acc'] >= 0.85 else '‚ö†Ô∏è BELOW TARGET'}")
    print(f"  ‚úì Recall (Sensitivity): {best_results['Recall']:.2%}")
    print(f"  ‚úì Precision: {best_results['Precision']:.2%}")
    print(f"  ‚úì ROC-AUC: {best_results['ROC_AUC']:.2%}")
    print(f"  ‚úì F1-Score: {best_results['F1_Score']:.2%}")
    
    print(f"\nüìÅ OUTPUT FILES:")
    print(f"  - 01_feature_distributions.png")
    print(f"  - 02_correlation_matrix.png")
    print(f"  - 03_model_comparison.png")
    print(f"  - 04_roc_curves.png")
    print(f"  - 05_confusion_matrices.png")
    print(f"  - diabetes_model.pkl (deployment-ready)")
    
    print("\n" + "=" * 80)
    print("Thank you for using the Diabetes Prediction ML Pipeline!")
    print("=" * 80)


if __name__ == "__main__":
    main()